---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an [NSERC CGS-D](https://www.nserc-crsng.gc.ca/Students-Etudiants/PG-CS/CGSD-BESCD_eng.asp) scholarship funded PhD student at [York University](https://www.yorku.ca/) in the [Computational Vision and Imaging Lab](https://yorkucvil.github.io/) supervised by the amazing [Dr. Kosta Derpanis](https://cs.ryerson.ca/~kosta/)! My research interest is mainly in interpreting the representations of deep neural networks with an emphasis on video understanding tasks. I am currently a research intern at [Ubisoft La Forge](https://www.ubisoft.com/en-us/studio/laforge) working on deep learning for character animations. I recently completed an internship at [Toyota Research Institute](https://www.tri.global/) as a member of the machine learning team working on [interpretability for video transformers](https://arxiv.org/abs/2401.10831). I am also a faculty affiliate researcher at the [Vector Institute](https://vectorinstitute.ai/) and have previously held the position of Lead Scientist in Residence at [NextAI](https://www.nextcanada.com/next-ai/) from 2020-2022. 

Before my PhD studies, I completed a Bachelors of Applied Science (B.A.Sc) in [Applied Mathematics and Engineering](https://www.queensu.ca/mathstat/mthe) with a specialization in Mechanical Engineering at [Queen's University](https://www.queensu.ca/) in Kingston, Ontario, Canada. After graduating from my Bachelors degree, I worked at [Morrison Hershfield](https://www.morrisonhershfield.com) as a mechanical design engineer in Toronto. I helped with the design of buildings, labratories, and condos, with a team of other mechanical, evironmental, electrical, and control engineers. I then obtained my M.Sc at the [Ryerson Vision Lab](https://ryersonvisionlab.github.io/) in August 2020 under the co-supervision of [Dr. Neil Bruce]([https://cs.ryerson.ca/~bruce/](https://socs.uoguelph.ca/~brucen/)) and [Dr. Konstantinos Derpanis](https://cs.ryerson.ca/~kosta/). My M.Sc thesis focused on quantifying the utility of different modalities for action recognition. 

My hobbies include health and fitness, competitive Super Smash Bros. Melee, birds, close up magic, and progressive house music.


## Project Highlights

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/vcc.png" alt="Project 1 Image" style="height: 300px; width: auto; border-radius: 15px;">
  <div>
    Visual Concept Connectome <strong>(Spotlight @ CVPR 2024)</strong> <br>
    Unsupervised discovery of concepts and their interlayer connections. <br>
    <a href="https://arxiv.org/abs/2404.02233">Paper</a>, 
    <a href="https://yorkucvil.github.io/VCC">project page</a>, 
    <a href="https://mkowal2.github.io/VCC_Demo/">demo</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/vtcd.png" alt="Project 2 Image" style="height: 300px; width: auto; border-radius: 15px;">
  <div>
    Understanding Video Transformers via Universal Concept Discovery <strong>(Spotlight @ CVPR 2024)</strong> <br>
    We discover universal spatiotemporal concepts in video transformers. <br>
    <a href="https://arxiv.org/abs/2401.10831">Paper</a>, 
    <a href="https://yorkucvil.github.io/VTCD">project page</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/staticdynamic.png" alt="Project 3 Image" style="height: 300px; width: auto; border-radius: 15px;">
  <div>
    A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information <strong>(@ CVPR 2022 and Spotlight @ <a href="https://xai4cv.github.io/workshop">XAI4CV Workshop</a>)</strong> <br>
    We develop a new metric for quantifying static and dynamic information in deep spatiotemporal models. <br>
    <a href="https://arxiv.org/abs/2206.02846">Paper</a>, 
    <a href="https://yorkucvil.github.io/Static-Dynamic-Interpretability/">project page</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/pooling.png" alt="Project 4 Image" style="height: 300px; width: auto; border-radius: 15px;">
  <div>
    Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs <strong>(@ ICCV 2021)</strong> <br>
    We show how spatial position information is encoded along the channel dimensions after pooling layers. <br>
    <a href="https://arxiv.org/abs/2108.07884">Paper</a>, 
    <a href="https://github.com/islamamirul/PermuteNet">code</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/shape2.png" alt="Project 5 Image" style="height: 300px; width: auto; border-radius: 15px;">
  <div>
    Shape or Texture: Understanding Discriminative Features in CNNs <strong>(@ ICLR 2021)</strong> <br>
    We develop a new metric for shape and texture information encoded in CNNs. <br>
    <a href="https://arxiv.org/abs/2101.11604">Paper</a>, 
    <a href="https://islamamirul.github.io/shape_neurons.html">project page</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/feature.png" alt="Project 6 Image" style="height: 300px; width: auto; border-radius: 15px;">
  <div>
    Feature Binding with Category-Dependent MixUp for Semantic Segmentation and Adversarial Robustness <strong>(Oral @ BMVC 2020)</strong> <br>
    Source separation augmentation improves semantic segmentation and robustness. <br>
    <a href="https://arxiv.org/abs/2008.05667">Paper</a>.
  </div>
</div>



## News 
- Gave an invited talk at [David Bau's Lab](https://baulab.info/) at Northeastern University!
- Gave an invited talk at [Thomas Serre's Lab](https://serre-lab.clps.brown.edu/person/thomas-serre/) at Brown University!
- Paper accepted to TPAMI! Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks.
 [Paper](https://arxiv.org/abs/2211.01783)
- TWO papers accepted as Highlights at CVPR 2024!
- CVPR 2024 paper accepted as a Highlight, a result of my Internship at [Toyota Research Institute](https://www.tri.global/) - Understanding Video Transformers via Universal Concept Discovery. [Paper](https://arxiv.org/abs/2401.10831) and [project page](https://yorkucvil.github.io/VTCD)! We will also be presenting this work as a poster at the [Causal and Object-Centric Representations for Robotics Workshop](https://corrworkshop.github.io/)
- CVPR 2024 paper accepted as a Highlight - Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models. [Paper](https://arxiv.org/abs/2404.02233) and [project page](https://yorkucvil.github.io/VCC). We will also be presenting this work as a poster at the [CVPR Explainable AI for Computer Vision Workshop](https://xai4cv.github.io/workshop_cvpr24)
- CAIC 2024 long paper accepted - Multi-modal News Understanding with Professionally Labelled Videos (ReutersViLNews)
. [Paper](https://arxiv.org/abs/2401.12419). 
- Paper accepted to the International Journal of Computer Vision (IJCV) - Position, Padding and Predictions: A Deeper Look at Position Information in CNNs
. [Paper](https://arxiv.org/abs/2101.12322). 
- I have been awarded the [NSERC CGS-D](https://www.nserc-crsng.gc.ca/Students-Etudiants/PG-CS/CGSD-BESCD_eng.asp) Scholarship with a total value of $105,000! (Accepted)
- I have accepted an offer to do a research internship at [Toyota Research Institute](https://www.tri.global/) for the Summer of 2023 at the Palo Alto HQ office!
- I gave a talk at [Vector's Endless Summer School program](https://vectorinstitute.ai/programs-courses/endless-summer-school/) on Current Trends in Computer Vision and a CVPR 2022 Recap
- Paper accepted to the International Journal of Computer Vision (IJCV) - SegMix: Co-occurrence Driven Mixup for Semantic
Segmentation and Adversarial Robustness. [Paper](https://arxiv.org/abs/2108.09929). 
- I presented a spolight presentation at the [Explainable AI for Computer Vision Workshop](https://xai4cv.github.io/workshop) at CVPR 2022. You can [watch the recorded talk here](https://www.youtube.com/watch?v=gpnmRG4aMHw&ab_channel=mkowal2).
- Paper Accpted to CVPR 2022 - A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information. [Paper](https://arxiv.org/abs/2206.02846) and [Project Page](https://yorkucvil.github.io/Static-Dynamic-Interpretability/).
- Paper Accepted to ICCV 2021 - Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs. [Paper](https://arxiv.org/abs/2108.07884).
- Paper Accepted to BMVC 2021 - Simpler Does It: Generating Semantic Labels with Objectness Guidance. [Paper](https://arxiv.org/abs/2110.10335).
- Paper Accepted to ICLR 2021 - Shape or Texture: Understanding Discriminative Features in CNNs. [Paper](https://arxiv.org/abs/2101.11604).
- Paper Accepted as an Oral to BMVC 2020 - Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness. [Paper](https://arxiv.org/abs/2008.05667).
