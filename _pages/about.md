---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a Research Resident at [FAR AI](https://far.ai/), exploring critical challenges in AI safety and mechanistic interpretability, with a particular interest in uncovering how large language models (LLMs) persuade and influence people. I have a PhD (funded by an [NSERC CGS-D](https://www.nserc-crsng.gc.ca/Students-Etudiants/PG-CS/CGSD-BESCD_eng.asp) scholarship) from [York University](https://www.yorku.ca/), where I was part of the [CVIL Lab](https://yorkucvil.github.io/) under the supervision of [Dr. Kosta Derpanis](https://csprofkgd.github.io/). My doctoral research focused on concept-based interpretability in multi-modal and video understanding systems.

My journey in AI research includes impactful industry experiences, such as internships at [Ubisoft La Forge](https://www.ubisoft.com/en-us/studio/laforge), where I worked on generative modeling for character animations, and at [Toyota Research Institute](https://www.tri.global/), contributing to [interpretability research for video transformers](https://arxiv.org/abs/2401.10831) within the machine learning team. Additionally, I hold a position as a faculty affiliate researcher at the [Vector Institute](https://vectorinstitute.ai/) and previously served as Lead Scientist in Residence at [NextAI](https://www.nextcanada.com/next-ai/) (2020–2022).

My academic path began with a Bachelor of Applied Science (B.A.Sc) in [Applied Mathematics and Engineering](https://www.queensu.ca/mathstat/mthe), specialized in Mechanical Engineering from [Queen's University](https://www.queensu.ca/) in Kingston, Ontario. Following graduation, I gained practical engineering experience at [Morrison Hershfield](https://www.morrisonhershfield.com), collaborating in multidisciplinary teams to design buildings, laboratories, and residential projects.

I then earned my Master's degree at the [Ryerson Vision Lab](https://ryersonvisionlab.github.io/) in August 2020, co-supervised by [Dr. Neil Bruce](https://socs.uoguelph.ca/~brucen/) and [Dr. Kosta Derpanis](https://csprofkgd.github.io/). My thesis focused on evaluating the effectiveness of various modalities in recognizing human actions.

Outside of research, I am a chronic hobbyist. I enjoy maintaining an active lifestyle focused on health and fitness through weightlifting and calisthenics, engaging competitively in Super Smash Bros. Melee, cooking, skateboarding, rock climbing, birdwatching, close-up magic, and immersing myself in the progressive house and techno music scene.

## Project Highlights

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/APE.png" alt="Project 2 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong>Preprint, 2025</strong> <br>
    It’s the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics <br>
    We introduce the AttemptPersuadeEval (APE) and show that frontier models attempt to persuade users into harmful topics. <br>
    <a href="https://www.arxiv.org/abs/2506.02873">Paper</a>,
    <a href="https://github.com/AlignmentResearch/AttemptPersuadeEval">Eval Code</a>
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/USAE.png" alt="Project 1 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong>ICML 2025</strong> <br>
    Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment <br>
    We train SAEs to discover universal and unique concepts across different models. <br>
    <a href="https://arxiv.org/abs/2502.03714">Paper</a>,
    <a href="https://github.com/YorkUCVIL/UniversalSAE">Code</a>
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/Arch.png" alt="Project 1 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong>ICML 2025</strong> <br>
    Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models <br>
    We design SAEs that solve the issue of stability across different training runs <br>
    <a href="https://arxiv.org/abs/2502.12892">Paper</a>,
    <a href="https://github.com/KempnerInstitute/overcomplete">Code</a>
  </div>
</div>


<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/vcc.png" alt="Project 1 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong>Spotlight @ CVPR 2024</strong> <br>
    Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models <br>
    Unsupervised discovery of concepts and their interlayer connections. <br>
    <a href="https://arxiv.org/abs/2404.02233">Paper</a>, 
    <a href="https://yorkucvil.github.io/VCC">project page</a>, 
    <a href="https://mkowal2.github.io/VCC_Demo/">demo</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/vtcd.png" alt="Project 2 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong>Spotlight @ CVPR 2024</strong>  <br>
    Understanding Video Transformers via Universal Concept Discovery  <br>
    We discover universal spatiotemporal concepts in video transformers. <br>
    <a href="https://arxiv.org/abs/2401.10831">Paper</a>, 
    <a href="https://yorkucvil.github.io/VTCD">project page</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/staticdynamic.png" alt="Project 3 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong> CVPR 2022 and Spotlight @ <a href="https://xai4cv.github.io/workshop">XAI4CV Workshop </a> </strong> <br>
    A Deeper Dive Into What Deep Spatiotemporal Networks Encode <br>
    We develop a new metric for quantifying static and dynamic information in deep spatiotemporal models. <br>
    <a href="https://arxiv.org/abs/2206.02846">Paper</a>, 
    <a href="https://yorkucvil.github.io/Static-Dynamic-Interpretability/">project page</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/pooling.png" alt="Project 4 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong> ICCV 2021 </strong> <br>
    Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs  <br>
    We show how spatial position information is encoded along the channel dimensions after pooling layers. <br>
    <a href="https://arxiv.org/abs/2108.07884">Paper</a>, 
    <a href="https://github.com/islamamirul/PermuteNet">code</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/shape2.png" alt="Project 5 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong> ICLR 2021</strong> <br>
    Shape or Texture: Understanding Discriminative Features in CNNs  <br>
    We develop a new metric for shape and texture information encoded in CNNs. <br>
    <a href="https://arxiv.org/abs/2101.11604">Paper</a>, 
    <a href="https://islamamirul.github.io/shape_neurons.html">project page</a>.
  </div>
</div>

<div style="display: flex; align-items: center; gap: 15px; margin-bottom: 20px;">
  <img src="./images/feature.png" alt="Project 6 Image" style="height: auto; width: 350px; border-radius: 15px;">
  <div>
    <strong>Oral @ BMVC 2020</strong> <br>
    Feature Binding with Category-Dependent MixUp for Semantic Segmentation and Adversarial Robustness <br>
    Source separation augmentation improves semantic segmentation and robustness. <br>
    <a href="https://arxiv.org/abs/2008.05667">Paper</a>.
  </div>
</div>

## News 
- I have officially defended my PhD on August 6th, 2025!
- Two papers accepted to ICML 2025 improving intepretability methods with SAEs! [Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment](https://arxiv.org/abs/2502.03714) and [Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models](https://arxiv.org/abs/2502.12892)!
- I joined [FAR.AI](https://far.ai/) as a Research Resident working on understanding LLM persuasive capabilities!
- Gave an invited talk at [David Bau's Lab](https://baulab.info/) at Northeastern University!
- Gave an invited talk at [Thomas Serre's Lab](https://serre-lab.clps.brown.edu/person/thomas-serre/) at Brown University!
- Paper accepted to TPAMI! Quantifying and Learning Static vs. Dynamic Information in Deep Spatiotemporal Networks.
 [Paper](https://arxiv.org/abs/2211.01783)
- TWO papers accepted as Highlights at CVPR 2024!
- CVPR 2024 paper accepted as a Highlight, a result of my Internship at [Toyota Research Institute](https://www.tri.global/) - Understanding Video Transformers via Universal Concept Discovery. [Paper](https://arxiv.org/abs/2401.10831) and [project page](https://yorkucvil.github.io/VTCD)! We will also be presenting this work as a poster at the [Causal and Object-Centric Representations for Robotics Workshop](https://corrworkshop.github.io/)
- CVPR 2024 paper accepted as a Highlight - Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models. [Paper](https://arxiv.org/abs/2404.02233) and [project page](https://yorkucvil.github.io/VCC). We will also be presenting this work as a poster at the [CVPR Explainable AI for Computer Vision Workshop](https://xai4cv.github.io/workshop_cvpr24)
- CAIC 2024 long paper accepted - Multi-modal News Understanding with Professionally Labelled Videos (ReutersViLNews)
. [Paper](https://arxiv.org/abs/2401.12419). 
- Paper accepted to the International Journal of Computer Vision (IJCV) - Position, Padding and Predictions: A Deeper Look at Position Information in CNNs
. [Paper](https://arxiv.org/abs/2101.12322). 
- I have been awarded the [NSERC CGS-D](https://www.nserc-crsng.gc.ca/Students-Etudiants/PG-CS/CGSD-BESCD_eng.asp) Scholarship with a total value of $105,000! (Accepted)
- I have accepted an offer to do a research internship at [Toyota Research Institute](https://www.tri.global/) for the Summer of 2023 at the Palo Alto HQ office!
- I gave a talk at [Vector's Endless Summer School program](https://vectorinstitute.ai/programs-courses/endless-summer-school/) on Current Trends in Computer Vision and a CVPR 2022 Recap
- Paper accepted to the International Journal of Computer Vision (IJCV) - SegMix: Co-occurrence Driven Mixup for Semantic
Segmentation and Adversarial Robustness. [Paper](https://arxiv.org/abs/2108.09929). 
- I presented a spolight presentation at the [Explainable AI for Computer Vision Workshop](https://xai4cv.github.io/workshop) at CVPR 2022. You can [watch the recorded talk here](https://www.youtube.com/watch?v=gpnmRG4aMHw&ab_channel=mkowal2).
- Paper Accpted to CVPR 2022 - A Deeper Dive Into What Deep Spatiotemporal Networks Encode: Quantifying Static vs. Dynamic Information. [Paper](https://arxiv.org/abs/2206.02846) and [Project Page](https://yorkucvil.github.io/Static-Dynamic-Interpretability/).
- Paper Accepted to ICCV 2021 - Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs. [Paper](https://arxiv.org/abs/2108.07884).
- Paper Accepted to BMVC 2021 - Simpler Does It: Generating Semantic Labels with Objectness Guidance. [Paper](https://arxiv.org/abs/2110.10335).
- Paper Accepted to ICLR 2021 - Shape or Texture: Understanding Discriminative Features in CNNs. [Paper](https://arxiv.org/abs/2101.11604).
- Paper Accepted as an Oral to BMVC 2020 - Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness. [Paper](https://arxiv.org/abs/2008.05667).
